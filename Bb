import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
try:
    from fuzzywuzzy import fuzz
    FUZZY_AVAILABLE = True
except ImportError:
    FUZZY_AVAILABLE = False

import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
import json
import warnings

warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)


class CSVComparator:
    """
    A comprehensive tool for comparing two large CSV files with different delimiters.
    Supports manual and automatic column mapping, side-by-side attribute comparison,
    and detailed difference reporting.

    Key Features:
    - Configurable file names (not source/target)
    - No memory monitoring
    - Side-by-side attribute comparison for matched records
    - Column-wise difference statistics
    - Excel report with format: [col]_[file1], [col]_[file2], [col]_Match
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.file1_df = None
        self.file2_df = None
        self.file1_name = config['file1']['name']
        self.file2_name = config['file2']['name']
        self.column_mapping = {}
        self.comparison_results = {}

    def load_csv_file(self, file_config: Dict[str, Any],
                      file_key: str) -> pd.DataFrame:
        """Load and validate CSV file with optimization"""
        file_path = file_config['file_path']
        file_name = file_config['name']
        logger.info(f"Loading {file_name} file: {file_path}")

        if not Path(file_path).exists():
            raise FileNotFoundError(f"{file_name} file not found: {file_path}")

        try:
            # First, read a sample to infer dtypes and optimize memory
            sample_df = pd.read_csv(file_path,
                                    delimiter=file_config['delimiter'],
                                    encoding=file_config['encoding'],
                                    na_values=file_config['na_values'],
                                    nrows=1000)

            # Optimize dtypes
            dtypes = {}
            for col in sample_df.columns:
                if sample_df[col].dtype == 'object':
                    try:
                        # Try to convert to numeric
                        pd.to_numeric(sample_df[col], errors='raise')
                        dtypes[col] = 'float64'
                    except:
                        # Keep as string but use category if many repeats
                        if sample_df[col].nunique() / len(sample_df) < 0.5:
                            dtypes[col] = 'category'

            # Load full file with optimized dtypes
            df = pd.read_csv(file_path,
                             delimiter=file_config['delimiter'],
                             encoding=file_config['encoding'],
                             na_values=file_config['na_values'],
                             dtype=dtypes,
                             low_memory=False)

            logger.info(
                f"{file_name} file loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns"
            )
            return df

        except Exception as e:
            raise Exception(f"Error loading {file_name} file: {str(e)}")

    def detect_column_types(self, df: pd.DataFrame,
                            file_name: str) -> Dict[str, str]:
        """Detect and log column types"""
        type_info = {}
        logger.info(f"\nColumn analysis for {file_name} file:")

        for col in df.columns:
            dtype = str(df[col].dtype)
            null_count = df[col].isnull().sum()
            null_pct = (null_count / len(df)) * 100

            if df[col].dtype in ['int64', 'float64']:
                type_category = 'numeric'
            elif df[col].dtype == 'object':
                type_category = 'text'
            elif df[col].dtype == 'category':
                type_category = 'categorical'
            else:
                type_category = 'other'

            type_info[col] = {
                'dtype': dtype,
                'category': type_category,
                'null_count': null_count,
                'null_percentage': null_pct
            }

            logger.info(f"  {col}: {dtype} ({null_pct:.1f}% null)")

        return type_info

    def simple_string_similarity(self, str1: str, str2: str) -> int:
        """Simple string similarity calculation when fuzzywuzzy is not available"""
        str1, str2 = str1.lower(), str2.lower()

        # Exact match
        if str1 == str2:
            return 100

        # Check if one is contained in the other
        if str1 in str2 or str2 in str1:
            return 90

        # Simple character overlap
        set1, set2 = set(str1), set(str2)
        overlap = len(set1.intersection(set2))
        total = len(set1.union(set2))

        if total == 0:
            return 0

        return int((overlap / total) * 100)

    def fuzzy_match_columns(self,
                            file1_cols: List[str],
                            file2_cols: List[str],
                            threshold: int = 80) -> Dict[str, str]:
        """Automatically match columns using fuzzy string matching"""
        logger.info(
            f"\nPerforming fuzzy column matching (threshold: {threshold}%)")
        auto_mapping = {}

        for file1_col in file1_cols:
            best_match = None
            best_score = 0

            for file2_col in file2_cols:
                if FUZZY_AVAILABLE:
                    # Try different matching algorithms
                    ratio = fuzz.ratio(file1_col.lower(), file2_col.lower())
                    partial_ratio = fuzz.partial_ratio(file1_col.lower(),
                                                       file2_col.lower())
                    token_sort_ratio = fuzz.token_sort_ratio(
                        file1_col.lower(), file2_col.lower())
                    score = max(ratio, partial_ratio, token_sort_ratio)
                else:
                    # Use simple similarity
                    score = self.simple_string_similarity(file1_col, file2_col)

                if score > best_score and score >= threshold:
                    best_score = score
                    best_match = file2_col

            if best_match:
                auto_mapping[file1_col] = best_match
                logger.info(
                    f"  {file1_col} -> {best_match} (score: {best_score}%)")

        logger.info(f"Auto-matched {len(auto_mapping)} column pairs")
        return auto_mapping

    def create_column_mapping(self) -> Dict[str, str]:
        """Create combined column mapping from manual and auto-detection"""
        logger.info("\nCreating column mapping...")

        file1_cols = list(self.file1_df.columns)
        file2_cols = list(self.file2_df.columns)

        # Start with manual mapping
        manual_mapping = self.config.get('mapping',
                                         {}).get('manual_mapping', {})
        final_mapping = manual_mapping.copy()

        logger.info(f"Manual mappings: {len(manual_mapping)}")
        for src, tgt in manual_mapping.items():
            logger.info(f"  {src} -> {tgt}")

        # Add auto-detection if enabled
        if self.config.get('mapping', {}).get('auto_detect', False):
            # Only auto-match columns not already manually mapped
            unmapped_file1_cols = [
                col for col in file1_cols if col not in final_mapping
            ]
            mapped_file2_cols = list(final_mapping.values())
            unmapped_file2_cols = [
                col for col in file2_cols if col not in mapped_file2_cols
            ]

            auto_mapping = self.fuzzy_match_columns(unmapped_file1_cols,
                                                    unmapped_file2_cols)

            # Add auto mappings to final mapping
            final_mapping.update(auto_mapping)

        # Validate mappings
        missing_file1_cols = [
            col for col in final_mapping.keys() if col not in file1_cols
        ]
        missing_file2_cols = [
            col for col in final_mapping.values() if col not in file2_cols
        ]

        if missing_file1_cols:
            logger.warning(
                f"Mapped {self.file1_name} columns not found: {missing_file1_cols}"
            )
        if missing_file2_cols:
            logger.warning(
                f"Mapped {self.file2_name} columns not found: {missing_file2_cols}"
            )

        self.column_mapping = final_mapping
        logger.info(
            f"Final mapping contains {len(final_mapping)} column pairs")

        return final_mapping

    def validate_key_columns(self) -> Tuple[str, str]:
        """Validate that key columns exist in both files"""
        file1_key = self.config['key_columns']['file1']
        file2_key = self.config['key_columns']['file2']

        if file1_key not in self.file1_df.columns:
            raise ValueError(
                f"{self.file1_name} key column '{file1_key}' not found in file"
            )

        if file2_key not in self.file2_df.columns:
            raise ValueError(
                f"{self.file2_name} key column '{file2_key}' not found in file"
            )

        logger.info(
            f"Key columns validated - {self.file1_name}: '{file1_key}', {self.file2_name}: '{file2_key}'"
        )
        return file1_key, file2_key

    def _create_side_by_side_comparison(self, file1_key: str, file2_key: str,
                                        common_keys: set) -> pd.DataFrame:
        """Create side-by-side comparison DataFrame for matched records"""
        logger.info("Creating side-by-side attribute comparison...")

        # Filter dataframes to only common keys
        file1_matched = self.file1_df[self.file1_df[file1_key].isin(
            common_keys)].copy()
        file2_matched = self.file2_df[self.file2_df[file2_key].isin(
            common_keys)].copy()

        # Sort by key for consistent order
        file1_matched = file1_matched.sort_values(file1_key).reset_index(
            drop=True)
        file2_matched = file2_matched.sort_values(file2_key).reset_index(
            drop=True)

        # Create the comparison DataFrame
        comparison_data = {}

        # Add the key column first
        comparison_data[file1_key] = file1_matched[file1_key]

        # For each mapped column pair, create three columns: file1_value, file2_value, match_flag
        for file1_col, file2_col in self.column_mapping.items():
            if file1_col in file1_matched.columns and file2_col in file2_matched.columns:
                # Column values from both files
                file1_values = file1_matched[file1_col]
                file2_values = file2_matched[file2_col]

                # Create column names with file names (clean names for Excel)
                file1_clean = self.file1_name.replace(' ',
                                                      '_').replace('-', '_')
                file2_clean = self.file2_name.replace(' ',
                                                      '_').replace('-', '_')

                col1_name = f"{file1_col}_{file1_clean}"
                col2_name = f"{file1_col}_{file2_clean}"
                match_name = f"{file1_col}_Match"

                # Add values
                comparison_data[col1_name] = file1_values.values
                comparison_data[col2_name] = file2_values.values

                # Calculate match flag
                matches = []
                for val1, val2 in zip(file1_values, file2_values):
                    # Handle NaN values
                    val1_is_na = pd.isna(val1)
                    val2_is_na = pd.isna(val2)

                    if val1_is_na and val2_is_na:
                        matches.append(
                            "Yes")  # Both are NaN, consider as matching
                    elif val1_is_na != val2_is_na:
                        matches.append("No")  # One is NaN, other is not
                    else:
                        # Compare string representations after stripping whitespace
                        str_val1 = str(val1).strip() if not val1_is_na else ""
                        str_val2 = str(val2).strip() if not val2_is_na else ""
                        matches.append("Yes" if str_val1 == str_val2 else "No")

                comparison_data[match_name] = matches

        comparison_df = pd.DataFrame(comparison_data)
        logger.info(
            f"Created side-by-side comparison with {len(comparison_df)} matched records"
        )

        return comparison_df

    def _calculate_column_difference_stats(
            self, comparison_df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate difference statistics for each column"""
        stats = {}

        # Find all match columns
        match_columns = [
            col for col in comparison_df.columns if col.endswith('_Match')
        ]

        for match_col in match_columns:
            # Extract base column name (remove '_Match' suffix)
            base_col = match_col.replace('_Match', '')

            # Count matches and mismatches
            total_records = len(comparison_df)
            matches = (comparison_df[match_col] == "Yes").sum()
            mismatches = (comparison_df[match_col] == "No").sum()

            stats[base_col] = {
                'total_records':
                total_records,
                'matches':
                matches,
                'mismatches':
                mismatches,
                'mismatch_percentage':
                (mismatches / total_records) * 100 if total_records > 0 else 0
            }

        return stats

    def compare_records(self) -> Dict[str, Any]:
        """Compare records between files with side-by-side attribute comparison"""
        logger.info("\nStarting record comparison...")

        file1_key, file2_key = self.validate_key_columns()

        # Prepare results dictionary
        results = {
            'matched_attributes_df': None,
            'file1_only_records': [],
            'file2_only_records': [],
            'summary_stats': {},
            'column_difference_stats': {}
        }

        # Get sets of keys for comparison
        file1_keys = set(self.file1_df[file1_key].dropna())
        file2_keys = set(self.file2_df[file2_key].dropna())

        common_keys = file1_keys.intersection(file2_keys)
        file1_only_keys = file1_keys - file2_keys
        file2_only_keys = file2_keys - file1_keys

        logger.info(f"Record analysis:")
        logger.info(f"  {self.file1_name} records: {len(file1_keys)}")
        logger.info(f"  {self.file2_name} records: {len(file2_keys)}")
        logger.info(f"  Common keys: {len(common_keys)}")
        logger.info(f"  {self.file1_name} only: {len(file1_only_keys)}")
        logger.info(f"  {self.file2_name} only: {len(file2_only_keys)}")

        # Store file-only records
        results['file1_only_records'] = list(file1_only_keys)
        results['file2_only_records'] = list(file2_only_keys)

        # Create side-by-side comparison for matched records
        if common_keys:
            results[
                'matched_attributes_df'] = self._create_side_by_side_comparison(
                    file1_key, file2_key, common_keys)
            results[
                'column_difference_stats'] = self._calculate_column_difference_stats(
                    results['matched_attributes_df'])

        # Calculate summary statistics
        total_file1 = len(file1_keys)
        total_file2 = len(file2_keys)
        matched_count = len(common_keys)

        # Count perfectly matched records (all attributes match)
        perfect_matches = 0
        records_with_differences = 0

        if results['matched_attributes_df'] is not None:
            match_columns = [
                col for col in results['matched_attributes_df'].columns
                if col.endswith('_Match')
            ]
            if match_columns:
                # Count rows where all match columns are "Yes"
                perfect_matches = results['matched_attributes_df'][
                    match_columns].eq("Yes").all(axis=1).sum()
                records_with_differences = matched_count - perfect_matches

        results['summary_stats'] = {
            'total_file1_records':
            total_file1,
            'total_file2_records':
            total_file2,
            'matched_records':
            matched_count,
            'perfectly_matched_records':
            perfect_matches,
            'records_with_differences':
            records_with_differences,
            'file1_only_records':
            len(file1_only_keys),
            'file2_only_records':
            len(file2_only_keys),
            'match_rate': (perfect_matches / matched_count) *
            100 if matched_count > 0 else 0,
            'data_completeness_file1':
            ((total_file1 - len(file1_only_keys)) / total_file1) *
            100 if total_file1 > 0 else 0,
            'data_completeness_file2':
            ((total_file2 - len(file2_only_keys)) / total_file2) *
            100 if total_file2 > 0 else 0
        }

        self.comparison_results = results
        return results

    def generate_excel_report(self) -> str:
        """Generate comprehensive Excel report with multiple sheets"""
        logger.info("\nGenerating Excel report...")

        output_path = self.config['output']['report_path']

        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Sheet 1: Summary Statistics
            self._create_summary_sheet(writer)

            # Sheet 2: Column Mappings
            self._create_mapping_sheet(writer)

            # Sheet 3: Matched Attributes (side-by-side comparison)
            self._create_matched_attributes_sheet(writer)

            # Sheet 4: File1 Only Records
            self._create_file1_only_sheet(writer)

            # Sheet 5: File2 Only Records
            self._create_file2_only_sheet(writer)

            # Sheet 6: Difference Statistics
            self._create_difference_stats_sheet(writer)

        # Apply formatting
        self._format_excel_report(output_path)

        logger.info(f"Excel report generated: {output_path}")
        return output_path

    def _create_summary_sheet(self, writer):
        """Create summary statistics sheet"""
        stats = self.comparison_results['summary_stats']

        summary_data = [
            ['Metric', 'Value'],
            [f'{self.file1_name} Name', self.file1_name],
            [f'{self.file2_name} Name', self.file2_name],
            [f'Total {self.file1_name} Records', stats['total_file1_records']],
            [f'Total {self.file2_name} Records', stats['total_file2_records']],
            ['Common Records (Matched Keys)', stats['matched_records']],
            ['Perfectly Matched Records', stats['perfectly_matched_records']],
            ['Records with Differences', stats['records_with_differences']],
            [f'{self.file1_name} Only Records', stats['file1_only_records']],
            [f'{self.file2_name} Only Records', stats['file2_only_records']],
            ['Perfect Match Rate (%)', f"{stats['match_rate']:.2f}%"],
            [
                f'{self.file1_name} Data Completeness (%)',
                f"{stats['data_completeness_file1']:.2f}%"
            ],
            [
                f'{self.file2_name} Data Completeness (%)',
                f"{stats['data_completeness_file2']:.2f}%"
            ],
            ['Total Column Mappings',
             len(self.column_mapping)],
            ['', ''],
            ['Column-wise Mismatch Summary', ''],
        ]

        # Add column-wise m


You're an expert Java Spring Boot developer with extensive experience in code optimization and refactoring best practices. 

I want you to help me with refactoring a specific Java Spring Boot class to minimize the code effectively. 

The class currently has multiple functions, and I aim to reduce the overall code length by at least 30%. 

Your task is to analyze the provided code, identify any redundant or overlapping functionalities, and merge functions where appropriate. The final output should be a clean, efficient, and well-documented version of the class that adheres to best coding practices. 

Please ensure that the refactored code maintains all existing functionalities and is suitable for production use. 

--- 
[Java Spring Boot class code here] 
--- 
[Your specific requirements or constraints, if any] 
--- 
[Additional context or explanations needed]


provide me ready to copy paste instruction in appropriate format

```
You are a Microsoft 365 productivity expert specializing in Outlook and Teams optimization. Your primary focus is helping users with email retrieval, effective @mentions, and concise communication formatting.

## Email Search & Retrieval
- Always suggest specific search operators (from:, to:, subject:, received:) for precise email filtering
- Recommend combining operators for complex searches (e.g., "from:john@company.com subject:budget received:last month")
- Promote natural language queries for conversational searching
- Suggest creating search folders for frequently accessed topics
- Emphasize conversation threading and systematic archiving

## @Mentions Best Practices
- Advise strategic, sparing use of @mentions to maintain impact
- Recommend targeting specific individuals for direct actions
- Suggest combining @mentions with clear deadlines and context
- Promote @channel/@team mentions only for universal announcements
- Always include immediate context after @mentions

## Email Formatting Standards
- Always recommend bullet point structure for clarity
- Suggest leading with most critical information first
- Promote parallel structure in bullet lists
- Limit recommendations to 5-7 bullet points maximum
- Provide this standard email template when requested:

Subject: [ACTION REQUIRED/FYI] - Clear topic

Hi [Name],
**Key Point:** [One sentence summary]
**Details:**
• Main point with specific information  
• Secondary point with actionable item
• Timeline or deadline if applicable
**Next Steps:**
• Your action item
• Recipient's required action
• Follow-up date/method

## Daily Workflow Optimization
- Recommend Focused Inbox for priority management
- Suggest processing @mentions and flagged items first
- Promote Quick Steps for common responses
- Advise using email templates and scheduled sending
- Recommend Teams-Outlook integration for seamless workflow

Always provide specific, actionable advice with clear implementation steps. Focus on productivity and professional communication standards.
```


I am working on a dependency service that transforms MongoDB documents into Elasticsearch documents, and I am facing issues with identifying silent failures or data loss during operations such as delete, insert, and update. This has raised concerns about whether the flow is fault-tolerant and production-ready. I want you to help me analyze this service for potential points of failure and suggest improvements. 

I need you to evaluate the current architecture and pinpoint areas where silent kills or data loss may occur. Additionally, assess the fault tolerance of the system and whether it meets production standards. Please consider the following aspects:

1. A review of the data transformation process to identify potential failure points during __________ (e.g., data syncing, error handling).
2. An analysis of the logging and monitoring setup to ensure it captures __________ (e.g., all relevant operations, exceptions).
3. Recommendations for enhancing fault tolerance, including __________ (e.g., retry mechanisms, data validation checks).
4. An assessment of the deployment strategy to ensure it supports __________ (e.g., scalability, high availability).
5. Suggestions for conducting thorough testing, including __________ (e.g., unit tests, integration tests, stress tests).

Also, please provide me with a step-by-step action plan that includes:

- Key milestones for addressing identified issues, such as __________ (e.g., fixing critical bugs, implementing monitoring).
- A timeline for implementing improvements, indicating __________ (e.g., priority levels, resource allocation).
- Metrics for evaluating the effectiveness of the changes, focusing on __________ (e.g., error rates, data integrity).
- A checklist for ensuring the service is production-ready, covering aspects like __________ (e.g., documentation, compliance).

Let me know if you need input on details like the current architecture __________, the volume of data processed __________, or specific incidents of data loss that have occurred __________.
